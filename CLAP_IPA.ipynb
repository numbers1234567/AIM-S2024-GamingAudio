{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lingjzhu/clap-ipa/tree/main\n",
    "import librosa\n",
    "import numpy as np\n",
    "clap_ipa_sr = 16000\n",
    "some_audio,sampling_rate = librosa.load(\"tutorials/audio-samples/angelo-reloading.mp3\")\n",
    "some_audio = librosa.resample(some_audio, orig_sr=sampling_rate, target_sr=16000)\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty array \n",
    "audio_array = np.empty([])\n",
    "\n",
    "word_detected = False\n",
    "\n",
    "\n",
    "actions = []\n",
    "scores = [0.998,0.01,0.22]\n",
    "for x in macro_functions.keys():\n",
    "    actions.append(x)\n",
    "\n",
    "#actions = [\"nothing\",\"clap\",\"snap\",]\n",
    "\n",
    "\n",
    "#CLAP model's prediction function\n",
    "def classify(audio):\n",
    "    global audio_array, word_detected, actions\n",
    "    i = 0\n",
    "    \n",
    "    samplerate, array = audio\n",
    "    \n",
    "    #converts the input live audio array to numpy array and appends it to current audio array\n",
    "    live_array = np.array(array)\n",
    "    audio_array = np.append(audio_array, live_array)\n",
    "    \n",
    "    dimension = audio_array.shape\n",
    "    \n",
    "    \n",
    "    #removes 5 second of audio from array if it's longer than 5 seconds\n",
    "    if((dimension[0]) > (240000)):\n",
    "        audio_array = audio_array[-240000:]\n",
    "        print(\"Audio size: \", audio_array.shape)\n",
    "    \n",
    "    #Pass audio to model for predictions\n",
    "    result = pipe(audio_array, candidate_labels=actions)\n",
    "    \n",
    "    #Formats the result to a dataframe and retrives the highest scoring label\n",
    "    df = pd.DataFrame(result)\n",
    "    max = df.idxmax(numeric_only=True)\n",
    "    index = int(max[0])\n",
    "    word = df.iat[index,1]\n",
    "    print(df)\n",
    "    print(word)\n",
    "    \n",
    "    #Checks if the highest scored label was an action label\n",
    "    if word != \"nothing\":\n",
    "        word_detected = True\n",
    "        print(\"word was detected: \", word)\n",
    "        print(\"pressing macros...\")\n",
    "        macro_functions[word]()\n",
    "        \n",
    "   \n",
    "    #if word is detected or if the audio reaches 5 seconds, \n",
    "    #then reset the audio array to empty\n",
    "    if (word_detected == True):\n",
    "        audio_array = np.empty([1,1])\n",
    "        print(\"All contents of audio array was removed.\")\n",
    "        word_detected = False\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07146668434143066\n",
      "tensor([0.0451])\n",
      "0.0760946273803711\n",
      "tensor([0.1298])\n",
      "0.07309556007385254\n",
      "tensor([0.5351])\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"./clap-ipa\")\n",
    "from clap.encoders import *\n",
    "import torch.nn.functional as F\n",
    "from transformers import DebertaV2Tokenizer, AutoProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "speech_encoder = SpeechEncoder.from_pretrained('anyspeech/clap-ipa-tiny-speech')\n",
    "phone_encoder = PhoneEncoder.from_pretrained('anyspeech/clap-ipa-tiny-phone')\n",
    "phone_encoder.eval().to(device)\n",
    "speech_encoder.eval().to(device)\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('charsiu/IPATokenizer')\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-tiny')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def ipa_converted_vals(labels, audio):\n",
    "    ipa_val_list = []\n",
    "\n",
    "    for x in range(len(labels)):\n",
    "        start = time.time()\n",
    "        audio_input = processor(audio,return_attention_mask=True,sampling_rate=16000,max_length=32000,return_tensors='pt')\n",
    "        ipa_input = tokenizer(ipa.convert(labels[x], keep_punct=True), return_token_type_ids=False, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            speech_embed = speech_encoder(**audio_input).pooler_output\n",
    "            phone_embed = phone_encoder(**ipa_input).pooler_output\n",
    "\n",
    "        similarity = F.cosine_similarity(speech_embed,phone_embed,dim=-1)\n",
    "        ipa_val_list.append(similarity)\n",
    "\n",
    "    return ipa_val_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIM-S2024-AudioGaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
